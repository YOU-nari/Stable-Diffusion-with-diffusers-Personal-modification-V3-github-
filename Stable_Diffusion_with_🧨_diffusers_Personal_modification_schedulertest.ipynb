{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YOU-nari/Stable-Diffusion-with-diffusers-Personal-modification-V3-github-/blob/main/Stable_Diffusion_with_%F0%9F%A7%A8_diffusers_Personal_modification_schedulertest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python„ÅÆ„Éê„Éº„Ç∏„Éß„É≥„Çí‰∏ä„Åí„Åü„ÅÑÊôÇ„Å†„ÅëÂÆüË°å\n",
        "\n",
        "‚ÄªÁèæÁä∂„ÅØÂÆüË°å„Åô„Çã„Å®Âãï„Åã„Å™„Åè„Å™„Çã„ÄÇ\n",
        "\n",
        "Â∞ÜÊù•‰Ωø„ÅÜ„Åã„ÇÇ„Åó„Çå„Å™„ÅÑ„ÅÆ„Åß‰∏ÄÂøúËøΩÂä†„Åó„Å¶„Åä„Åè"
      ],
      "metadata": {
        "id": "odHVwbiCa0qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title OS„ÅÆ„Éê„Éº„Ç∏„Éß„É≥Á¢∫Ë™ç\n",
        "!lsb_release -a"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xdqm5_4_PGFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Python„ÅÆ„Éê„Éº„Ç∏„Éß„É≥„Éª„Éë„ÇπÁ¢∫Ë™ç\n",
        "!echo $PYTHONPATH\n",
        "!python --version"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cyNINCcoP3pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ‰Ωø„ÅÑ„Åü„ÅÑPython„ÅÆ„Éê„Éº„Ç∏„Éß„É≥„ÇíÊåáÂÆö\n",
        "targetPy:str=\"3.9\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "dzNPgYmsQPYi",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Miniconda„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´\n",
        "\n",
        "#@markdown [Miniconda„ÅÆ„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Éö„Éº„Ç∏](https://docs.conda.io/en/latest/miniconda.html)„Å´Ë°å„Åç„ÄÅLinuxÁî®„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É©„ÅÆÂêçÂâç„Å®URL„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åä„Åè„ÄÇ\n",
        "\n",
        "#@markdown ‰ª•‰∏ã„ÅÆ„Çª„É´„Å´‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å´Ë®òËºâ„Åô„ÇãÔºà**Á©∫ÁôΩÂé≥Á¶Å**Ôºâ\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "#@markdown - `MINOCONDA_INSTALLER`=`sh„Éï„Ç°„Ç§„É´Âêç`„ÄÄÔºà„Å®„Çä„ÅÇ„Åà„Åö„ÄÅÊúÄÊñ∞ÁâàÔºâ\n",
        "#@markdown - `MINICONDA_DOWNLOAD_HP`=`URL`\n",
        "\n",
        "#@markdown ÔºàÊúÄÁµÇÁ¢∫Ë™çÔºö2021/7/3Ôºâ\n",
        "%%bash\n",
        "MINICONDA_INSTALLER=Miniconda3-py39_4.12.0-Linux-x86_64.sh #@param {type:\"string\"}\n",
        "MINICONDA_DOWNLOAD_HP=https://repo.anaconda.com/miniconda #@param {type:\"string\"}\n",
        "\n",
        "MINICONDA_PREFIX=/usr/local\n",
        "wget $MINICONDA_DOWNLOAD_HP/$MINICONDA_INSTALLER\n",
        "chmod +x $MINICONDA_INSTALLER\n",
        "./$MINICONDA_INSTALLER -b -f -p $MINICONDA_PREFIX"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eP7qf_T9Q5x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ‰ª•‰∏ã„ÅÆ„Çª„É´„ÇíÂÆüË°å„Åó„Å¶`conda „Éê„Éº„Ç∏„Éß„É≥`„ÅåË°®Á§∫„Åï„Çå„Åü„ÇâOK\n",
        "!conda -V"
      ],
      "metadata": {
        "cellView": "form",
        "id": "n_mawP3gTJNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Miniconda„Çí„Ç¢„ÉÉ„Éó„Éá„Éº„Éà\n",
        "%%bash\n",
        "conda init bash\n",
        "conda update -n base -c defaults conda -y"
      ],
      "metadata": {
        "cellView": "form",
        "id": "L3OQSsqIUaBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Python„ÅÆ„Éê„Éº„Ç∏„Éß„É≥„ÇíÂ§âÊõ¥„Å®Á¢∫Ë™ç\n",
        "!conda install python=$targetPy -y\n",
        "!python -V"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lNe9bX7SVuZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title „Ç§„É≥„Éù„Éº„Éà„É¢„Ç∏„É•„Éº„É´„ÇíÊ§úÁ¥¢„Åï„Åõ„Çã„Åü„ÇÅ„Å´`sys.path`„Å∏„Éë„Çπ„ÇíËøΩÂä†\n",
        "import sys\n",
        "sys.path.append(\"/usr/local/lib/python\"+targetPy+\"/site-packages\")\n",
        "sys.path"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cW_OMoX5V86X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "„Åì„Åì„Åæ„Åß"
      ],
      "metadata": {
        "id": "eWv5VLqzhbE6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpvjXRDm_QQq",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Âà©Áî®ÂèØËÉΩ„Å™GPU„Å®VRAM„ÅÆÁ¢∫Ë™ç\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3PJ33S-Ktra"
      },
      "source": [
        "‚ÜëGPU„ÅåÂãï„Åè‰∫ãÁ¢∫Ë™ç„Åó„Å¶„Å≠     \n",
        "„ÇÇ„ÅóÂãï„ÅÑ„Å¶„Å™„ÅÑÂ†¥Âêà„ÅØ „É©„É≥„Çø„Ç§„É†„ÅÆ„Çø„Ç§„Éó„ÇíGPU„Å´„Åó„Å¶„Å≠\n",
        "\n",
        "‚Üìdiffusers„ÅåÊõ¥Êñ∞„Åï„Çå„Åü„ÇâÂ§âÊõ¥„Åó„Å¶„Å≠\n",
        "ÁèæÊôÇÁÇπÊúÄÊñ∞\n",
        "diffusion:1.5\n",
        "diffuser:0.7.2\n",
        "Waifu:1.3\n",
        "trinart:v2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title colabo„ÅÆÊÆã„ÇäÊôÇÈñìÁ¢∫Ë™ç\n",
        "!cat /proc/uptime | awk '{printf(\"ÊÆã„ÇäÊôÇÈñì : %.2f\", 12-$1/60/60)}'"
      ],
      "metadata": {
        "cellView": "form",
        "id": "z0RjMtSqhiUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQEwlgJXKHmE"
      },
      "outputs": [],
      "source": [
        "#@title Stable Diffusion diffusers„Çí„É≠„Éº„Éâ\n",
        "\n",
        "!pip install -q diffusers==0.7.2 transformers scipy ftfy accelerate\n",
        "!pip install -q transformers scipy ftfy\n",
        "!pip install -q --upgrade diffusers[torch]\n",
        "\n",
        "#!huggingface-cli login\n",
        "#Ëá™ÂàÜ„ÅÆ„Éà„Éº„ÇØ„É≥Ë®òËºâ diffuser0.4.0„Åã„Çâ„ÅØ1Â∫¶„É≠„Ç∞„Éª„Ç§„É≥„Åô„Çå„Å∞‰∏çË¶Å„Çâ„Åó„ÅÑ\n",
        "#YOUR_TOKEN=\"\"\n",
        "!git clone https://github.com/openai/triton.git\n",
        "%cd triton/python/\n",
        "!pip install -q -e .\n",
        "!pip3 install -q --upgrade triton\n",
        "!pip install pytorch_lightning\n",
        "%pip install -q https://github.com/metrolobo/xformers_wheels/releases/download/1d31a3ac_various_6/xformers-0.0.14.dev0-cp37-cp37m-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title HuggingFaceü§ó„Å´„É≠„Ç∞„Ç§„É≥\n",
        "\n",
        "#markdown  https://huggingface.co/settings/tokens\n",
        "!mkdir -p ~/.huggingface\n",
        "HUGGINGFACE_TOKEN = \"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\" #param {type:\"string\"}\n",
        "!echo -n \"{HUGGINGFACE_TOKEN}\" > ~/.huggingface/token"
      ],
      "metadata": {
        "id": "ps9nDqIAWyxQ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz8TYP62PxF0"
      },
      "source": [
        "‚ÜìSEED„ÅÆ„É©„É≥„ÉÄ„É†Âåñ„Å®„ÄÅStable Diffusion„É¢„Éá„É´„ÅÆÈÅ∏Êäû„ÇíËøΩÂä†\n",
        "„É¢„Éá„É´„ÇíÂàá„ÇäÊõø„Åà„ÇãÊôÇ„ÅØ\"„É©„É≥„Çø„Ç§„É†„ÅÆÊé•Á∂ö„ÇíËß£Èô§„Åó„Å¶„Åã„ÇâÂÜçÂÆüË°å"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mgw5A58WIkyc",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "9f0eb897ad9e45b5ae1649da0d1b86cb",
            "7aae4876f10041958cad5e8612f1e295",
            "59224d2e3afe4a45b24c7f3848270660",
            "610f1c9599514a6f82f008fe3ab07afd",
            "d0a05e0b60ae4e048a8356efccf49e5e",
            "1fb9449e2ae54f4aba26776cab1b07f0",
            "9f13f02691be44e4b0dddbdcde1ff90c",
            "0509872b363643be97e411cbb0d5e90e",
            "f5d6aae7c38d4659b0703e8e505e49ad",
            "409a899a67e54e06bbdcb672b2fc5b4f",
            "e4dc2d5bdbd542c3863781b4259ec531"
          ]
        },
        "outputId": "f430fcbe-9cab-4e29-a894-e03f498bdefd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'visual_projection.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'logit_scale', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'text_projection.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight']\n",
            "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f0eb897ad9e45b5ae1649da0d1b86cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:Waife\n"
          ]
        }
      ],
      "source": [
        "#@title ÂøÖË¶Å„Å™„Éá„Éº„Çø„ÅÆ„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„ÇÑË™≠Ëæº\n",
        "\n",
        "from transformers import CLIPTokenizer ,CLIPTextModel\n",
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers.models import AutoencoderKL\n",
        "from diffusers import UNet2DConditionModel\n",
        "\n",
        "#„Éà„Éº„ÇØ„Éä„Ç§„Ç∫„Å®„ÉÜ„Ç≠„Çπ„Éà„ÅÆ„Ç®„É≥„Ç≥„Éº„ÉâÁî®„Å´„ÄÅtokenizer„Å®„ÄÅtext_encoder„ÇíË™≠„ÅøËæº„ÇÄ\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "#SEED„Çí„É©„É≥„ÉÄ„É†Âåñ„Åó„Åü„ÅÑ„ÅÆ„ÅßËøΩÂä†\n",
        "import random\n",
        "\n",
        "#Xformers„ÅÆÂÆüË£Ö„Çí„Åó„Åü„ÅÑ„Åë„Å©„Çà„Åè„Çè„Åã„Çâ„Çì„Å™\n",
        "#!sudo docker run -it --gpus=all --ipc=host -v /home:/home nvcr.io/nvidia/pytorch:22.08-py3 bash\n",
        "#!pip install --pre torch\n",
        "#!pip install xformers pytorch_lightning numpy\n",
        "#!pip3 install triton\n",
        "#!git clone https://github.com/openai/triton.git\n",
        "#%cd triton/python/\n",
        "#!pip install -e .\n",
        "\n",
        "#!pip install pytorch_lightning\n",
        "# Then \n",
        "# !pip install git+https://github.com/facebookresearch/xformers@51dd119#egg=xformers\n",
        "\n",
        "# Followed by\n",
        "#!cd PATH_TO_DIFFUSER_FOLDER\n",
        "#!git checkout memory_efficient_attention\n",
        "#  !pip install -e . \n",
        "\n",
        "#import math\n",
        "#import os\n",
        "\n",
        "#import pytorch_lightning as pl\n",
        "#import torch\n",
        "#import torch.nn as nn\n",
        "\n",
        "#from pytorch_lightning import Trainer, seed_everything\n",
        "#from pytorch_lightning.utilities import rank_zero_info\n",
        "#from torch.nn import functional as F\n",
        "#from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
        "\n",
        "#from xformers.factory.model_factory import xFormer, xFormerConfig\n",
        "\n",
        "#„Å®„Çä„ÅÇ„Åà„Åö„Ç≥„É≥„Éë„Ç§„É´Ê∏à„Åø„Åì„ÇåÂÖ•„Çå„Å®„Åç„ÇÉÂãï„Åè„Åã„Çâ„ÅÑ„ÅÑ„Åã\n",
        "#%pip install -q https://github.com/metrolobo/xformers_wheels/releases/download/1d31a3ac_various_6/xformers-0.0.14.dev0-cp37-cp37m-linux_x86_64.whl\n",
        "# These were compiled on Tesla T4, should also work on P100, thanks to https://github.com/metrolobo\n",
        "\n",
        "#@markdown Stable Diffusion„É¢„Éá„É´„ÇíÈÅ∏Êäû\n",
        "\n",
        "#@markdown Normal,Waife,Trinart,Trinart-Waife-50-50„Åã„ÇâÈÅ∏Êäû\n",
        "model = \"Waifu Diffusion\" #@param [\"\",\"Stable Diffusion\",\"Waifu Diffusion\",\"Trinart Stable Diffusion\",\"Trinart Waifu Stable Diffusion 50-50\"]\n",
        "\n",
        "if model == \"Stable Diffusion\":\n",
        " from diffusers import StableDiffusionPipeline #, {use_scheduler} #EulerAncestralDiscreteScheduler\n",
        "\n",
        "# pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", use_auth_token=YOUR_TOKEN\n",
        "# ).to(\"cuda\")\n",
        "\n",
        "# ÊΩúÂú®Á©∫Èñì„ÇíÁîªÂÉèÁ©∫Èñì„Å´„Éá„Ç≥„Éº„Éâ„Åô„Çã„Åü„ÇÅ„ÅÆVAE„É¢„Éá„É´„ÇíË™≠„ÅøËæº„ÇÄ\n",
        " #vae = AutoencoderKL.\n",
        " vae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"vae\")\n",
        "# ÊΩúÂú®Á©∫Èñì„ÇíÁîüÊàê„Åô„Çã„Åü„ÇÅ„ÅÆU-Net„É¢„Éá„É´„ÅÆÊåáÂÆö\n",
        " unet = UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"unet\") \n",
        " pipe = StableDiffusionPipeline.from_pretrained(\n",
        "     \"runwayml/stable-diffusion-v1-5\", scheduler=scheduler, torch_dtype=torch.float16 ,revision=\"fp16\", vae=vae,custom_pipeline=\"lpw_stable_diffusion\",\n",
        " ).to(\"cuda\")\n",
        "\n",
        "#NSFWÂõûÈÅøÂá¶ÁêÜ\n",
        " def dummy(images, **kwargs): return images, False\n",
        " pipe.safety_checker = dummy\n",
        " print(\"Model:Normal\")\n",
        "\n",
        "elif model == \"Waifu Diffusion\":\n",
        " from diffusers import StableDiffusionPipeline,LMSDiscreteScheduler #DDIMScheduler\n",
        " vae = AutoencoderKL.from_pretrained(\"hakurei/waifu-diffusion\", subfolder=\"vae\")\n",
        " unet = UNet2DConditionModel.from_pretrained(\"hakurei/waifu-diffusion\", subfolder=\"unet\")\n",
        "\n",
        "# StableDiffusion„Éë„Ç§„Éó„É©„Ç§„É≥„ÅÆÊ∫ñÂÇô 1.3„Åã„Çâscheduler„ÅåÂ§â„Çè„Å£„Åü„ÅÆ„Åß„Ç≥„É°„É≥„ÉàÂâç„ÅÆ„ÅØ„Ç≥„É°„É≥„Éà„Ç¢„Ç¶„Éà\n",
        " pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            \"hakurei/waifu-diffusion\",\n",
        "            torch_dtype=torch.float32,\n",
        "            #revision=\"fp16\",\n",
        "            #scheduler=DDIMScheduler\n",
        "            scheduler=LMSDiscreteScheduler(\n",
        "            beta_start=0.00085,\n",
        "            beta_end=0.012,\n",
        "            beta_schedule=\"scaled_linear\",\n",
        "            #clip_sample=False,\n",
        "            #set_alpha_to_one=False,\n",
        "            num_train_timesteps=1000\n",
        " ),\n",
        "            #use_auth_token=YOUR_TOKEN\n",
        " ).to(\"cuda\")\n",
        " def dummy(images, **kwargs): return images, False\n",
        " pipe.safety_checker = dummy\n",
        " print(\"Model:Waife\")\n",
        "\n",
        "elif model == \"Trinart Stable Diffusion\":\n",
        " from diffusers import StableDiffusionPipeline\n",
        " vae = AutoencoderKL.from_pretrained(\"naclbit/trinart_stable_diffusion_v2\", subfolder=\"vae\")\n",
        " unet = UNet2DConditionModel.from_pretrained(\"naclbit/trinart_stable_diffusion_v2\", subfolder=\"unet\")\n",
        "\n",
        "# StableDiffusion„Éë„Ç§„Éó„É©„Ç§„É≥„ÅÆÊ∫ñÂÇô\n",
        " pipe = StableDiffusionPipeline.from_pretrained(\n",
        "      \"naclbit/trinart_stable_diffusion_v2\", \n",
        "      revision=\"diffusers-60k\",\n",
        "      use_auth_token=YOUR_TOKEN\n",
        " ).to(\"cuda\")\n",
        " def dummy(images, **kwargs): return images, False\n",
        " pipe.safety_checker = dummy\n",
        " print(\"Model:Trinart\")\n",
        "\n",
        "elif model == \"Trinart Waifu Stable Diffusion 50-50\":\n",
        " from diffusers import StableDiffusionPipeline\n",
        " vae = AutoencoderKL.from_pretrained(\"doohickey/trinart-waifu-diffusion-50-50\", subfolder=\"vae\")\n",
        " unet = UNet2DConditionModel.from_pretrained(\"doohickey/trinart-waifu-diffusion-50-50\", subfolder=\"unet\")\n",
        "\n",
        "# StableDiffusion„Éë„Ç§„Éó„É©„Ç§„É≥„ÅÆÊ∫ñÂÇô\n",
        " pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        \"doohickey/trinart-waifu-diffusion-50-50\", \n",
        "      use_auth_token=YOUR_TOKEN\n",
        " ).to(\"cuda\")\n",
        " def dummy(images, **kwargs): return images, False\n",
        " pipe.safety_checker = dummy\n",
        " print(\"Model:Trinart-Waifu-50-50\")\n",
        "\n",
        "else:\n",
        " print(\"ÂÖ•Âäõ„Ç®„É©„Éº:Êï∞ÂÄ§„ÇíË¶ãÁõ¥„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\")\n",
        "\n",
        "# „É¢„Éá„É´„ÇíGPU„Å∏Áßª„Åô\n",
        "vae = vae.to(\"cuda\")\n",
        "text_encoder = text_encoder.to(\"cuda\")\n",
        "unet = unet.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXi-LvVVJRCB"
      },
      "source": [
        "„Åì„Åì„Åã„Çâ‰∏ã„ÅØÂ•Ω„ÅçÂãùÊâã„ÅÑ„Åò„Å£„Å¶ÈÅä„Å∂„Åü„ÇÅ„ÅÆ„Éë„É©„É°„Éº„Çø„Éº"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGfmRXeVjGyJ"
      },
      "source": [
        "num_inference_steps„ÅØ1~200\n",
        "\n",
        "ÂõûÊï∞„ÅåÂ§ö„ÅÑÁ®ãÁ∂∫È∫ó„Å´„Å™„Çã„Åå„ÄÅÊ•µÁ´Ø„Å´Â§ß„Åç„Åè„Åó„Å¶„ÇÇÂ§â„Çè„Çâ„Å™„ÅÑ\n",
        "\n",
        "Êé®Â•®ÂÄ§„ÅØ50\n",
        "\n",
        "guidance_scale„ÅØ1~20\n",
        "\n",
        "Êï∞ÂÄ§„ÅåÂ§ß„Åç„ÅÑÁ®ãPromptÂÜÖÂÆπ„Å´Ëøë„Åè„Å™„Çã„ÅåÂ§öÊßòÊÄß„ÅåÁÑ°„Åè„Å™„Çã\n",
        "\n",
        "Êé®Â•®ÂÄ§„ÅØ7„Äú8.5„Åè„Çâ„ÅÑ„ÄÅ30Ë∂Ö„Åà„Çã„Å®Á†¥Á∂ª„Åô„Çã\n",
        "\n",
        "batch_size Â¢ó„ÇÑ„Åô„Å®ÊñΩË°åÊï∞„ÅåÂ¢ó„Åà„Çã?\n",
        "\n",
        "ÂÆüË°åÈÄüÂ∫¶„ÇÇ‰∏ä„Åå„Çã„ÅåVRAM„ÇíÂ§ßÈáè„Å´Ê∂àË≤ª"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "m8tPqDxHJX9i",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Â§âÊï∞„ÅÆË®≠ÂÆö\n",
        "\n",
        "#Â§âÊï∞ÂÆ£Ë®Ä\n",
        "num_inference_steps = 10     #@param {type:\"number\"}      # Number of denoising steps\n",
        "guidance_scale = 7        #@param {type:\"number\"}       # Scale for classifier-free guidance\n",
        "#batch_size = 2 #@param {type:\"number\"}\n",
        "# ÁîªÂÉè„ÅÆ„Çµ„Ç§„Ç∫ 512*512„ÅåÂàùÊúüÂÄ§\n",
        "height = 512        #@param {type:\"number\"}              # default height of Stable Diffusion\n",
        "width = 512             #@param {type:\"number\"}          # default width of Stable Diffusion\n",
        "#batch_size = 1 #@param {type:\"number\"}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ÊèèÁîªÊåáÁ§∫\n",
        "\n",
        "#ÂÖ•ÂäõÊñáÂ≠ó „Åì„Åì„Å´Â•Ω„Åç„Å™Á¶ÅÂâáÊñáÂ≠ó„Çí„ÅÑ„Çå„Å¶„Åè„Å†„Åï„ÅÑ\n",
        "\n",
        "prompt = \"masterpiece, insane detaled, best quality, A woman in a luxury dress with long hair with a beautiful and perfect symmetrical face blonde loose perm by mucha\" #@param {type:\"string\"}\n",
        "\n",
        "#„Éç„Ç¨„ÉÜ„Ç£„Éñ Èô§Â§ñ„Åó„Åü„ÅÑË¶ÅÁ¥†„ÇíÂÖ•„Çå„Çã\n",
        "\n",
        "negative_prompt = \"pablo picasso,monet,dali,van gogh,\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "gwE8DaQ24Z0M",
        "cellView": "form"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title „Çπ„Ç±„Ç∏„É•„Éº„É©„ÅÆÈÅ∏Êäû\n",
        "#@markdown „Åü„Å†„Åó„ÄÅDDPM,KarrasVe„ÅØÁîªÂÉèÂá∫ÂäõÂá¶ÁêÜ„ÅåÂøÖË¶Å„Å™„ÅÆ„Åã„Éë„É©„É°„Éº„Çø„ÇíÂÄãÂà•Ë®≠ÂÆö„Åó„Å™„ÅÑ„Å®„ÅÑ„Åë„Å™„ÅÑ„ÅÆ„ÅãÁü•„Çâ„Çì„Åë„Å©Âãï„Åã„Å™„ÅÑ\n",
        "use_schedulers = \"EulerDiscreteScheduler\" #@param [\"\",\"DDIMScheduler\",\"DDPMScheduler\",\"KarrasVeScheduler\",\"LMSDiscreteScheduler\",\"EulerDiscreteScheduler\",\"EulerAncestralDiscreteScheduler\"]\n",
        "#use_scheduler Â∞ÜÊù•‰Ωø„ÅÜ„Åã„ÇÇ? scheduler„ÅÆÊåáÂÆöÁî®Â§âÊï∞„Å®„Åô„Çã„Åã„ÇÇÔºü\n",
        "\n",
        "# „Éé„Ç§„Ç∫„Çπ„Ç±„Ç∏„É•„Éº„É©„ÅÆÊåáÂÆö\n",
        "if use_schedulers == \"DDIMScheduler\":\n",
        " from diffusers import DDIMScheduler\n",
        " scheduler = DDIMScheduler.from_config(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\")\n",
        "\n",
        "elif use_schedulers == \"DDPMScheduler\":\n",
        " from diffusers import DDPMSchedulerduler\n",
        " scheduler = DDPMScheduler.from_config(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\")\n",
        "\n",
        "elif use_schedulers == \"KarrasVeScheduler\":\n",
        " from diffusers import KarrasVeScheduler\n",
        " scheduler = KarrasVeScheduler.from_config(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\")\n",
        "\n",
        "elif use_schedulers == \"LMSDiscreteScheduler\":\n",
        " from diffusers import LMSDiscreteScheduler\n",
        " scheduler = LMSDiscreteScheduler.from_config(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\")\n",
        "\n",
        "elif use_schedulers == \"EulerDiscreteScheduler\":\n",
        " from  diffusers import EulerDiscreteScheduler\n",
        " scheduler = EulerDiscreteScheduler.from_config(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\")\n",
        "\n",
        "elif use_schedulers == \"EulerAncestralDiscreteScheduler\":\n",
        " from  diffusers import EulerAncestralDiscreteScheduler\n",
        " scheduler = EulerAncestralDiscreteScheduler.from_config(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pDx_qVEDrmLw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "-j8-Yu00JGMA",
        "outputId": "f0c82b80-01f7-45f9-a0f2-540e6acb64ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-43-f052ce3d460f>\"\u001b[0;36m, line \u001b[0;32m21\u001b[0m\n\u001b[0;31m    images = pipe.text2img(prompt, negative_prompt = negative_prompt, width = width,height = height,guidance_scale = guidance_scale, num_inference_steps = num_inference_steps,max_embeddings_multiples = ,generator = generator).images\u001b[0m\n\u001b[0m                                                                                                                                                                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "20#@title ÁîªÂÉèÁîüÊàêÂõûÊï∞„Å®SEED„ÅÆÁ®ÆÂà•\n",
        "\n",
        "N = 1 #@param {type:\"number\"}\n",
        "seed = 1 #@param {type:\"number\"}\n",
        "seed_fix = True #@param {'type':'boolean'}\n",
        "for i in range(N):\n",
        "# SEEDÂÄ§„ÄÅ„Åì„Åì„Çí„Åã„Åà„Çã„Å® Âêå„ÅòÂÖ•ÂäõÊñáÂ≠ó„Åß„ÇÇÂà•„ÅÆÁîªÂÉè„Åå„Åß„Åæ„Åô\n",
        "# seed„ÇíÂõ∫ÂÆö„Åô„ÇãÊôÇ„ÅØseed_fix„Å´„ÉÅ„Çß„ÉÉ„ÇØ\n",
        "   if seed_fix == True:\n",
        "    seed = seed\n",
        "    generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "   else:\n",
        "    seed = random.randrange(0, 2147483647, 1)\n",
        "    generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "   with autocast(\"cuda\"):\n",
        "  #VRAMÊ∂àË≤ª„ÇíÊäë„Åà„Åü„ÅÑÂ†¥ÂêàÊúâÂäπ„Å´‚Üì\n",
        "    pipe.enable_attention_slicing()\n",
        "    pipe.enable_xformers_memory_efficient_attention()\n",
        "    with torch.inference_mode():\n",
        "    #images = pipe(prompt, height = height, width = width, guidance_scale = guidance_scale, num_inference_steps = num_inference_steps,negative_prompt = negative_prompt, generator = generator).images\n",
        "     images = pipe.text2img(prompt, negative_prompt = negative_prompt, width = width,height = height,guidance_scale = guidance_scale, num_inference_steps = num_inference_steps,max_embeddings_multiples=8,generator = generator).images[0]\n",
        "\n",
        "#„Å©„ÅÆSEEDÂÄ§„ÅßÊèèÁîª„Åï„Çå„Åü„ÅãÁ¢∫Ë™çÁî®\n",
        "     print(\"SEED =\",seed)\n",
        "#„Å©„ÅÆscheduler„Çí‰Ωø„Å£„Åü„Åã\n",
        "     print(\"SCHEDULER =\",use_scheduler)\n",
        "     display(images[0])#.save(f'output{i}.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o06TWdF3JQaA"
      },
      "source": [
        "„ÇÇ„Å£„Å®Ë©≥„Åó„ÅèÁü•„Çä„Åü„ÅÑ‰∫∫„ÅØ\n",
        "https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb#scrollTo=zHkHsdtnry57\n",
        "„Åì„Å£„Å°„ÅÆ„Å°„ÇÉ„Çì„Å®„Åó„Åücolab„ÇíË¶ã„Çã„Çì„Å†ÔºÅÔºÅÔºÅÔºÅ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhfQ4GLv39K3"
      },
      "source": [
        "‚ÜìWaifuÂçòÁã¨Âãï‰Ωú\n",
        "\n",
        "„Åì„Åì„Åã„ÇâÂãï„Åã„Åõ„Å∞ËâØ„ÅÑ„ÄÇ\n",
        "Áâπ„Å´2Ê¨°ÂÖÉ„ÅØWifu„ÅåÂúßÂÄíÁöÑ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ye_11709e3Y0"
      },
      "outputs": [],
      "source": [
        " #@title Waifu Diffusion„ÅÆ„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Å®Ë®≠ÂÆö\n",
        "!pip install -q diffusers==0.7.2 transformers scipy ftfy accelerate\n",
        "!pip install -q transformers scipy ftfy\n",
        "!pip install -q --upgrade diffusers[torch]\n",
        "\n",
        "from transformers import CLIPTokenizer ,CLIPTextModel\n",
        "\n",
        "#„Éà„Éº„ÇØ„Éä„Ç§„Ç∫„Å®„ÉÜ„Ç≠„Çπ„Éà„ÅÆ„Ç®„É≥„Ç≥„Éº„ÉâÁî®„Å´„ÄÅtokenizer„Å®„ÄÅtext_encoder„ÇíË™≠„ÅøËæº„ÇÄ\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "\n",
        "#Xformers„ÅÆÂÆüË£Ö„Çí„Åó„Åü„ÅÑ„Åë„Å©„Çà„Åè„Çè„Åã„Çâ„Çì„Å™\n",
        "# !sudo docker run -it --gpus=all --ipc=host -v /home:/home nvcr.io/nvidia/pytorch:22.08-py3 bash\n",
        "\n",
        "# Then \n",
        "# !pip install git+https://github.com/facebookresearch/xformers@51dd119#egg=xformers\n",
        "\n",
        "# Followed by\n",
        "# !cd PATH_TO_DIFFUSER_FOLDER\n",
        "# !git checkout memory_efficient_attention\n",
        "# !pip install -e . \n",
        "!git clone https://github.com/openai/triton.git\n",
        "%cd triton/python/\n",
        "!pip install -q -e .\n",
        "!pip3 install -q --upgrade triton\n",
        "%pip install -q https://github.com/metrolobo/xformers_wheels/releases/download/1d31a3ac_various_6/xformers-0.0.14.dev0-cp37-cp37m-linux_x86_64.whl\n",
        "\n",
        "#!pip install pytorch_lightning\n",
        "#„Å®„Çä„ÅÇ„Åà„Åö„Ç≥„É≥„Éë„Ç§„É´Ê∏à„Åø„Åì„ÇåÂÖ•„Çå„Å®„Åç„ÇÉÂãï„Åè„Åã„Çâ„ÅÑ„ÅÑ„Åã\n",
        "#!pip install -q https://github.com/metrolobo/xformers_wheels/releases/download/1d31a3ac_various_6/xformers-0.0.14.dev0-cp37-cp37m-linux_x86_64.whl\n",
        "# These were compiled on Tesla T4, should also work on P100, thanks to https://github.com/metrolobo\n",
        "\n",
        "#„É©„É≥„ÉÄ„É†Âåñ„Åó„Åü„ÅÑ„ÅÆ„ÅßËøΩÂä†\n",
        "import random\n",
        "import torch\n",
        "\n",
        "from diffusers.models import AutoencoderKL\n",
        "from diffusers import UNet2DConditionModel\n",
        "\n",
        "from torch import autocast\n",
        "#from diffusers import DiffusionPipeline\n",
        " from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler #DDIMScheduler\n",
        "# StableDiffusion„Éë„Ç§„Éó„É©„Ç§„É≥„ÅÆÊ∫ñÂÇô\n",
        "# pipe = StableDiffusionPipeline.from_pretrained(\n",
        "#  \"hakurei/waifu-diffusion\",\n",
        "#  torch_dtype=torch.float32,\n",
        "  #revision=\"fp16\",#‚ÜêÊúÄÊñ∞„Éê„Éº„Ç∏„Éß„É≥„ÅßÂªÉÊ≠¢„Åï„Çå„Å¶„Çã\n",
        "  #scheduler=DDIMScheduler(\n",
        "#vae = AutoencoderKL.from_pretrained(\"waifu-diffusion-v1-4/vae/kl-f8-anime.ckpt\")‚Üê‰ªÆ 1.4„É™„É™„Éº„ÇπÊôÇ„Å´Á¢∫Ë™ç\n",
        "vae = AutoencoderKL.from_pretrained(\"hakurei/waifu-diffusion\", subfolder=\"vae\")\n",
        "unet = UNet2DConditionModel.from_pretrained(\"hakurei/waifu-diffusion\", subfolder=\"unet\")\n",
        " pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    'hakurei/waifu-diffusion',   \n",
        "    custom_pipeline=\"lpw_stable_diffusion\",\n",
        "    #revision=\"fp32\",\n",
        "    torch_dtype=torch.float32,#vae=vae,\n",
        "  scheduler=LMSDiscreteScheduler(\n",
        "         beta_start=0.00085,\n",
        "         beta_end=0.012,\n",
        "         beta_schedule=\"scaled_linear\",\n",
        "         #.clip_sample=False,\n",
        "         #set_alpha_to_one=False,\n",
        "         num_train_timesteps=1000\n",
        "     ),\n",
        "     #use_auth_token=YOUR_TOKEN\n",
        " ).to(\"cuda\")\n",
        " vae = vae.to(\"cuda\")\n",
        " text_encoder = text_encoder.to(\"cuda\")\n",
        " unet = unet.to(\"cuda\")\n",
        "#NSFWÂõûÈÅøÂá¶ÁêÜ\n",
        "def dummy(images, **kwargs): return images, False\n",
        "pipe.safety_checker = dummy\n",
        "#VRAM‰ΩéÊ∂àË≤ª‚ÄªÂ∞ë„ÅóÂÆüË°åÈÄüÂ∫¶„ÅåËêΩ„Å°„Çã\n",
        "#pipe.enable_attention_slicing()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJTIRKLa_yfl"
      },
      "source": [
        "GUI„Åß„Å™„ÅèCUI„ÅßÂãï„Åè„Çà„ÅÜ„Å´Áõ¥„Åó„Åü„ÄÇ\n",
        "„É´„Éº„ÉóÁµÑ„Çì„ÅßÊåáÂÆö„Åó„ÅüÂõûÊï∞ÂÆüË°å„ÅóÁ∂ö„Åë„Çã„ÅÆ„Åß„Åì„Å£„Å°„ÅÆ„Åª„ÅÜ„Åå‰Ωø„ÅÑÂãùÊâã„Çà„ÅÑ„ÄÇ\n",
        "„É©„É≥„Çø„Ç§„É†ÂàáÊñ≠„Åß„ÇÇÂÜçËµ∑ÂãïÂæåÁ∂ôÁ∂ö„Åô„Çã„ÄÇ"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "num_inference_steps„ÅØ1~200\n",
        "\n",
        "ÂõûÊï∞„ÅåÂ§ö„ÅÑÁ®ãÁ∂∫È∫ó„Å´„Å™„Çã„Åå„ÄÅÊ•µÁ´Ø„Å´Â§ß„Åç„Åè„Åó„Å¶„ÇÇÂ§â„Çè„Çâ„Å™„ÅÑ\n",
        "\n",
        "Êé®Â•®ÂÄ§„ÅØ50\n",
        "\n",
        "guidance_scale„ÅØ1~20\n",
        "\n",
        "Êï∞ÂÄ§„ÅåÂ§ß„Åç„ÅÑÁ®ãPromptÂÜÖÂÆπ„Å´Ëøë„Åè„Å™„Çã„ÅåÂ§öÊßòÊÄß„ÅåÁÑ°„Åè„Å™„Çã\n",
        "\n",
        "Êé®Â•®ÂÄ§„ÅØ7„Äú8.5„Åè„Çâ„ÅÑ„ÄÅ30Ë∂Ö„Åà„Çã„Å®Á†¥Á∂ª„Åô„Çã\n",
        "\n",
        "batch_size Â¢ó„ÇÑ„Åô„Å®ÊñΩË°åÊï∞„ÅåÂ¢ó„Åà„Çã?\n",
        "\n",
        "ÂÆüË°åÈÄüÂ∫¶„ÇÇ‰∏ä„Åå„Çã„ÅåVRAM„ÇíÂ§ßÈáè„Å´Ê∂àË≤ª\n",
        "\n",
        "Âü∫Êú¨ÁöÑ„Å´„ÅØ512√ó512„Åå1Áï™ËâØ„ÅÑÁµêÊûú„ÅåÂá∫„Çã\n",
        "\n",
        "guidance_scale 1~30 Â§ß„Åç„ÅÑÁ®ãprompt„Å´Ëøë„ÅÑÁîªÂÉè„Å´„Å™„Çã„ÅåÂ§öÊßòÊÄß„ÅØÁÑ°„Åè„Å™„Çã"
      ],
      "metadata": {
        "id": "YDOetnwC1w1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Â§âÊï∞„ÅÆË®≠ÂÆö\n",
        "\n",
        "#Â§âÊï∞ ÂàùÊúüÂÄ§ 512*512 scale:6 step:50\n",
        "height = 768 #@param {type:\"number\"}\n",
        "width = 512 #@param {type:\"number\"}\n",
        "guidance_scale = 12 #@param {type:\"number\"}\n",
        "num_inference_steps = 50 #@param {type:\"number\"}\n",
        "#batch_size = 1 #@param {type:\"number\"}\n",
        "#num_samples = 1 „Åì„Çå„ÅØ [prompt] * num_samples„ÅßÂÖ•„ÇåÂ≠ê„Å´ÊåáÂÆöÂõûÊï∞ÂàÜÂá∫Âäõ"
      ],
      "metadata": {
        "id": "gPL1RDhR0X_8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ÊèèÁîªÊåáÁ§∫\n",
        "prompt = \"masterpiece, best quality, nsfw, uncensored, masterpiece, Insane detaled, watercolor 1, pastel colors, pov, looking at viewer, 1girl, solo, gsmell, (dwarf woman), fat, plump, very ugly, mature female, muscular female, looks 10 yo, cute dog ears, chonky female, fluffy, beautiful scarlet curl longhair, luxury naughty panty, luxury naughty bra, black pantyhose, panties over pantyhose, dog tail, animal tail, animal ears, animal hands, animal ears fluff, thick thighs, huge ass, huge breasts, huge nipples, puffy nipples, dark nipples, inverted nipples, anus, anal hair, many facial, many pubic hair, female orgasm, many pussy juice, spread pussy,reverse cowgirl position, spread legs, many cum in pussy, many cum on hair, many cum on body, many cum in mouth, breast grab, paizuri, fellatio, she is insert huge veiny penis, precum, he is ejaculating, she looks in agony, she looks painful, she has teary eyes, she looks hates, rape, sex, hetero, anal, vaginal, in a public Park\" #@param {type:\"string\"}\n",
        "\n",
        "negative_prompt = \"censored, pablo picasso, poorly drawn, ((futanari)), ((futa with female)), ((futa with futa)), ((futa with male)), ((2girls)), ((multiple girls)), ((multiple body)), ((multiple thighs)), ((multiple crotches)), ((multiple pussy)), ((multiple breasts)), ((multiple legs)), ((multiple ass)), ((multiple fingers )), (bad anatomy), the background is incoherent, more than 2 thighs, huge thighs, huge calf, missing hand, disappearing arms, disappearing thigh, disappearing calf, disappearing legs, missing fingers, one hand with more than 5 fingers, one hand with less than 5 fingers, extra digits, extra arms, extra legs, extra fingers extra penises, extra testicles, missing arms, missing legs, missing fingers, missing asshole, fused fingers, fused hand, fused asshole, fused arms, fused legs, fused fingers, fused anus, fused pussy, bad hands, bad asshole, bad anus, bad pussy, bad crotch, bad crotch seam, bad feet, (abnormal eye proportion:1.2), (Abnormal hands), (abnormal legs), (abnormal feet), image macro, ranguage, watermark, watermarked, long head, long body, mutated hands and fingers, speech bubble, blank speech bubble, shared speech bubble, speech stab, sound effects, typo, anatomical nonsense, quality, solid color thumbnail, fewer digits, lowres, worst quality, low quality, normal quality, jpeg artifacts, signature, username, blurry, text, error, (face out of frame), (cropped), black and white painting, black-white, monochrome, monotone, greyscale, lineart, retro style, 1980s, 1990s, comic, artist name, canvas, art frame low resolution, low detail, out of focus, bad framing, tiling\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "LdLCkaac0sN_",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Dc8wK6oTpRt"
      },
      "outputs": [],
      "source": [
        "8#@title „É´„Éº„ÉóÂõûÊï∞\n",
        "N = 1 #@param {type:\"number\"}\n",
        "seed = 1 #@param {type:\"number\"}\n",
        "seed_fix = False #@param {'type':'boolean'} \n",
        "#„É´„Éº„ÉóÂá¶ÁêÜ\n",
        "for i in range(N):\n",
        "   if seed_fix == True:\n",
        "    seed = seed\n",
        "    generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "   else:\n",
        "    seed = random.randrange(0, 2147483647, 1)\n",
        "    generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "   with autocast(\"cuda\"):\n",
        "    pipe.enable_attention_slicing()\n",
        "    pipe.enable_xformers_memory_efficient_attention()\n",
        "    with torch.inference_mode():\n",
        "    #images = pipe(prompt, height = height, width = width, guidance_scale = guidance_scale, num_inference_steps = num_inference_steps,negative_prompt = negative_prompt, generator = generator).images\n",
        "     images = pipe.text2img(prompt, negative_prompt = negative_prompt, width = width,height = height,guidance_scale = guidance_scale, num_inference_steps = num_inference_steps,max_embeddings_multiples=8,generator = generator).images[0]\n",
        "\n",
        "#„Å©„ÅÆSEEDÂÄ§„ÅßÊèèÁîª„Åï„Çå„Åü„ÅãÁ¢∫Ë™çÁî®\n",
        "     print(\"SEED =\",seed)\n",
        "     display(images)#.save(f'output{i}.png')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Gdrive„Åã„Çâ„É¢„Éá„É´„Éá„Éº„Çø„ÇíË™≠Ëæº„Åß„Åç„Çã„Çà„ÅÜ„Å´„Åó„Åü„ÅÑ\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!python -m venv .env\n",
        "\n",
        "!source .env/bin/activate\n",
        "\n",
        "!pip install diffusers==0.7.1 transformers scipy ftfy accelerate\n",
        "!pip install --upgrade diffusers transformers scipy\n",
        "#!huggingface-cli login\n",
        "import torch\n",
        "from torch import autocast\n",
        "# secretAI\n",
        "from diffusers import StableDiffusionPipeline, EulerAncestralDiscreteScheduler\n",
        "\n",
        "euler_ancestral_scheduler = EulerAncestralDiscreteScheduler.from_config(\"content/drive/MyDrive/model/novelAI.ckpt\", subfolder=\"scheduler\")\n",
        "pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "    \"content/drive/MyDrive/model/novelAI.ckpt\",torch_dtype=torch.float16, custom_pipeline=\"lpw_stable_diffusion\", scheduler=euler_scheduler, use_auth_token=YOUR_TOKEN,\n",
        ")\n",
        "pipeline.to(\"cuda\")"
      ],
      "metadata": {
        "id": "Z5cLeoAiY_fr",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9f0eb897ad9e45b5ae1649da0d1b86cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7aae4876f10041958cad5e8612f1e295",
              "IPY_MODEL_59224d2e3afe4a45b24c7f3848270660",
              "IPY_MODEL_610f1c9599514a6f82f008fe3ab07afd"
            ],
            "layout": "IPY_MODEL_d0a05e0b60ae4e048a8356efccf49e5e"
          }
        },
        "7aae4876f10041958cad5e8612f1e295": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fb9449e2ae54f4aba26776cab1b07f0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9f13f02691be44e4b0dddbdcde1ff90c",
            "value": "Fetching 15 files: 100%"
          }
        },
        "59224d2e3afe4a45b24c7f3848270660": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0509872b363643be97e411cbb0d5e90e",
            "max": 15,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f5d6aae7c38d4659b0703e8e505e49ad",
            "value": 15
          }
        },
        "610f1c9599514a6f82f008fe3ab07afd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_409a899a67e54e06bbdcb672b2fc5b4f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e4dc2d5bdbd542c3863781b4259ec531",
            "value": " 15/15 [00:00&lt;00:00, 178.45it/s]"
          }
        },
        "d0a05e0b60ae4e048a8356efccf49e5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fb9449e2ae54f4aba26776cab1b07f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f13f02691be44e4b0dddbdcde1ff90c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0509872b363643be97e411cbb0d5e90e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5d6aae7c38d4659b0703e8e505e49ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "409a899a67e54e06bbdcb672b2fc5b4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4dc2d5bdbd542c3863781b4259ec531": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}